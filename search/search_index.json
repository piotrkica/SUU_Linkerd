{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Linkerd-18: Linkerd Case Study","text":"<p>Linkerd is an cloud-native example of a service mesh -- which is a way to control how different parts of an application share data with one another. Unlike other systems for managing this communication, a service mesh is a dedicated infrastructure layer built right into an app. This visible infrastructure layer can document how well (or not) different parts of an app interact, so it becomes easier to optimize communication and avoid downtime as an app grows. 1</p> <ol> <li> <p>RedHat. What's a service mesh? June 29, 2018. URL: https://www.redhat.com/en/topics/microservices/what-is-a-service-mesh.\u00a0\u21a9</p> </li> </ol>"},{"location":"background/","title":"Background","text":"<p>Modern applications are often broken down as a network of services each performing a specific business function. In order to execute its function, one service might need to request data from several other services. But what if some services get overloaded with requests? This is where a service mesh comes in -- it routes requests from one service to the next, optimizing how all the moving parts work together.</p> <p>Service-to-service communication is what makes microservices possible. The logic governing communication can be coded into each service without a service mesh layer -- but as communication gets more complex, a service mesh becomes more valuable. For cloud-native apps built in a microservices architecture, a service mesh is a way to comprise a large number of discrete services into a functional application.</p> <p>A service mesh doesn't introduce new functionality to an app's runtime environment -- apps in any architecture have always needed rules to specify how requests get from point A to point B. What's different about a service mesh is that it takes the logic governing service-to-service communication out of individual services and abstracts it to a layer of infrastructure.</p> <p>In a service mesh, requests are routed between microservices through proxies in their own infrastructure layer. For this reason, individual proxies that make up a service mesh are sometimes called sidecars since they run alongside each service, rather than within them. Taken together, these sidecar proxies -- decoupled from each service -- form a mesh network. A sidecar proxy sits alongside a microservice and routes requests to other proxies. Together, these sidecars form a mesh network.</p> <p>Without a service mesh, each microservice needs to be coded with logic to govern service-to-service communication, which means developers are less focused on business goals. It also means communication failures are harder to diagnose because the logic that governs interservice communication is hidden within each service.</p> <p>Every new service added to an app, or new instance of an existing service running in a container, complicates the communication environment and introduces new points of possible failure. Within a complex microservices architecture, it can become nearly impossible to locate where problems have occurred without a service mesh.</p> <p>That's because a service mesh also captures every aspect of service-to-service communication as performance metrics. Over time, data made visible by the service mesh can be applied to the rules for interservice communication, resulting in more efficient and reliable service requests.</p> <p>For example, if a given service fails, a service mesh can collect data on how long it took before a retry succeeded. As data on failure times for a given service aggregates, rules can be written to determine the optimal wait time before retrying that service, ensuring that the system does not become overburdened by unnecessary retries.</p>"},{"location":"case_study_concept/","title":"Case study concept","text":"<p>Linkerd, with the use of proxy sidecars, is capable of service-to-service encryption. For that purpose it uses mTLS, which stands for mutual TLS. With the use of it every meshed service is capable of authenticating any request by a workload certificate derived from its Kubernetes ServiceAccount token.</p> <p>Without the usage of Linkerd, there is a risk of malicious eavesdropper which could sniff the communication, e.g. from a shared messaging platform thus exposing potentially sensitive data. Note that when using encryption provided by Linkerd, the risk is substantially lowered, which is pointed out in latest security audit 1. One of the points of this case study is to show the difference.</p> <p>We plan on using a \"sniffer\" to observe the encrytpion of the communication. Two services (further called service A and Service B) will be set up using Kubernetes. Then, ksniff 2 and Wireshark will be used to record communication between the pods, and to potentially observe data sent.</p> <p></p> <p>Then, the Linkerd service mesh will be deployed, and the experiment will be repeated. This time, the Linkerd should encrytp observed communication, thus making reading sent data impossible.</p> <p></p> <ol> <li> <p>William Morgan. Announcing the completion of linkerd's 2022 security audit. June 27, 2022. URL: https://linkerd.io/2022/06/27/announcing-the-completion-of-linkerds-2022-security-audit.\u00a0\u21a9</p> </li> <li> <p>Eldad Rudich. Ksniff. January 30, 2023. URL: https://github.com/eldadru/ksniff?fbclid=IwAR0_3IuIv36cqxW_xFZxbSsOTCjGRvST5kADJvqt53si9hvqveG4ShJLdm4.\u00a0\u21a9</p> </li> </ol>"},{"location":"ciphertext/","title":"Ciphertext","text":"<p>Mesh application:</p> <pre><code># Perform pre-checks for Linkerd installation\nlinkerd check --pre\n\n# Install Linkerd control plane CRDs\nlinkerd install --crds | kubectl apply -f -\n\n# Install Linkerd control plane components\nlinkerd install | kubectl apply -f - # Takes approximately 50 seconds to start running, 70 seconds for all components to start\n# (Optional) Restart the linkerd-destination pod in the linkerd namespace\nkubectl -n linkerd rollout restart linkerd-destination\n\n# Perform post-installation checks for Linkerd\nlinkerd check\n</code></pre> <p>Ignore warning:</p> <pre><code># Ignore warning by injecting Linkerd sidecar into server deployment YAML and applying the modified configuration\nkubectl get deployment server -o yaml | linkerd inject - | kubectl apply -f -\n\n# Ignore warning by injecting Linkerd sidecar into client deployment YAML and applying the modified configuration\nkubectl get deployment client -o yaml | linkerd inject - | kubectl apply -f -\n</code></pre> <p>Linkerd is starting new pods, search for new pods and their netnamespaces: find new client's name:</p> <pre><code># Find the name of the new client pod\nkubectl get pods -l app=client\n</code></pre> <p>Find new server's name:</p> <pre><code># Find the names of the new client and server pods\nkubectl get pods -l app=client\nkubectl get pods -l app=server\n\n# Copy the index3.html file to the server pod's nginx HTML directory using the nginx container\nkubectl -c nginx cp index3.html server-&lt;new...&gt;:usr/share/nginx/html/index.html\n\n# Execute the nginx reload command on the server pod using the nginx container\nkubectl -c nginx exec server-&lt;new...&gt; -- nginx -s reload\n</code></pre> <p>Write down <code>&lt;server-new-ip&gt;</code>:</p> <pre><code># Write down the IP address of the server pod\nkubectl get pods -l app=server -o wide\n</code></pre> <p>Find and copy netnamespace <code>&lt;server-new-ns&gt;</code> having <code>&lt;server-new-ip&gt;</code>:</p> <pre><code># Find and copy the netnamespace &lt;server-new-ns&gt; corresponding to &lt;server-new-ip&gt;\nsudo ip -all netns exec ip -c a\nkubectl -c nginx -i -t exec client-&lt;new...&gt; -- bash\neng@fedora:~$ sudo ip netns exec &lt;server-new-ns&gt; tcpdump -i eth0 -A port http\nroot@client-&lt;new...&gt;:~# curl &lt;cluster-ip-server&gt;\n</code></pre> <p></p>"},{"location":"configuration_setup/","title":"Configuration set-up","text":"<p>The following commands are intended for execution on a RHEL-compatible operating system, such as Fedora:</p> <pre><code>1. Enable the required modules to ensure proper functionality.\n2. Initiate the necessary services to enable desired functionalities.\n3. Safely terminate the firewall service to manage network access effectively.\n</code></pre> <pre><code># Enable necessary modules\nsudo modprobe br-netfilter\n\n# Enable IP forwarding\nsudo sysctl net.ipv4.ip_forward=1\n# Enable bridge-netfilter calls for iptables\nsudo sysctl net.bridge.bridge-nf-call-iptables=1\n# Enable bridge-netfilter calls for ip6tables\nsudo sysctl net.bridge.bridge-nf-call-ip6tables=1\n# Start containerd service\nsudo systemctl start containerd\n\n# Stop firewalld service\nsudo systemctl stop firewalld\n</code></pre> <p>Clean after previous installations:</p> <pre><code># Clean up previous installations\nsudo kubeadm reset\nsudo rm /etc/cni/net.d/10-flannel.conflist\n\n# Initialize the Kubernetes cluster with a specific pod network CIDR\nsudo kubeadm init --pod-network-cidr=10.244.0.0/16\n\n# Remove existing kubeconfig files\nrm ~/.kube/*\n</code></pre> <p>We will now proceed with the professional installation of a <code>k8s</code> cluster, ensuring adherence to best practices and following the official documentation for a seamless deployment.</p>"},{"location":"environment_configuration_description/","title":"Environment configuration description","text":""},{"location":"environment_configuration_description/#step-0-setup","title":"Step 0: Setup","text":"<p>Before anything else, we need to ensure you have access to modern Kubernetes cluster and a functioning kubectl command on your local machine. (If you don\u2019t already have a Kubernetes cluster, one easy option is to run one on your local machine. There are many ways to do this, including kind, k3d, Docker for Desktop, and more.)</p> <p>Validate your Kubernetes setup by running:</p> <pre><code>kubectl version --short\n</code></pre> <p>You should see output with both a Client Version and Server Version component.</p> <p>Now that we have our cluster, we\u2019ll install the Linkerd CLI and use it validate that your cluster is capable of hosting Linkerd.</p>"},{"location":"environment_configuration_description/#step-1-install-the-cli","title":"Step 1: Install the CLI","text":"<p>If this is your first time running Linkerd, you will need to download the linkerd CLI onto your local machine. The CLI will allow you to interact with your Linkerd deployment.</p> <p>To install the CLI manually, run:</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/install | sh\n</code></pre> <p>Be sure to follow the instructions to add it to your path.</p> <p>Once installed, verify the CLI is running correctly with:</p> <pre><code>linkerd version\n</code></pre> <p>You should see the CLI version, and also Server version: unavailable. This is because you haven\u2019t installed the control plane on your cluster. Don\u2019t worry\u2014we\u2019ll fix that soon enough.</p>"},{"location":"environment_configuration_description/#step-2-validate-your-kubernetes-cluster","title":"Step 2: Validate your Kubernetes cluster","text":"<p>Kubernetes clusters can be configured in many different ways. Before we can install the Linkerd control plane, we need to check and validate that everything is configured correctly. To check that your cluster is ready to install Linkerd, run:</p> <pre><code>linkerd check --pre\n</code></pre> <p>If there are any checks that do not pass, make sure to follow the provided links and fix those issues before proceeding.</p>"},{"location":"environment_configuration_description/#step-3-install-linkerd-onto-your-cluster","title":"Step 3: Install Linkerd onto your cluster","text":"<p>Now that you have the CLI running locally and a cluster that is ready to go, it\u2019s time to install Linkerd on your Kubernetes cluster. To do this, run:</p> <pre><code>linkerd install --crds | kubectl apply -f -\n</code></pre> <p>followed by:</p> <pre><code>linkerd install | kubectl apply -f -\n</code></pre> <p>These commands generate Kubernetes manifests with all the core resources required for Linkerd (feel free to inspect this output if you\u2019re curious). Piping these manifests into kubectl apply then instructs Kubernetes to add those resources to your cluster. The install --crds command installs Linkerd\u2019s Custom Resource Definitions (CRDs), which must be installed first, while the install command installs the Linkerd control plane.</p> <p>Depending on the speed of your cluster\u2019s Internet connection, it may take a minute or two for the control plane to finish installing. Wait for the control plane to be ready (and verify your installation) by running:</p> <p>linkerd check</p> <p>1</p> <ol> <li> <p>Getting started. URL: https://linkerd.io/2.13/getting-started/.\u00a0\u21a9</p> </li> </ol>"},{"location":"installation/","title":"Installation and execution procedure","text":"<p>Proceed with the professional installation of a Kubernetes (k8s) cluster by executing the following commands, which involve configuring the kubeconfig, applying the flannel network configuration, and tainting the node to remove control-plane and master roles::</p> <pre><code># Copy the Kubernetes admin configuration to the current user's kubeconfig\nsudo cp /etc/kubernetes/admin.conf ~/.kube/config\n\n# Change ownership of the kubeconfig file to the current user\nsudo chown &lt;user&gt;:&lt;user&gt; ~/.kube/config\n\n# Apply the flannel network configuration (replace &lt;flannel&gt; with the appropriate file)\nkubectl apply -f &lt;flannel&gt;\n\n# Taint the node to remove control-plane and master roles\nkubectl taint node fedora.36 node-role.kubernetes.io/control-plane-\nkubectl taint node fedora.36 node-role.kubernetes.io/master-\n</code></pre> <p>To remove the loop from the configuration, execute the following command:</p> <pre><code># Edit the CoreDNS configuration map\nKUBE_EDITOR=vim kubectl -n kube-system edit configmap coredns\n\n# Restart the CoreDNS pods\nkubectl -n kube-system rollout restart coredns\n</code></pre>"},{"location":"plaintext/","title":"Plaintext sniffing","text":"<pre><code># Create a deployment for the server with nginx image and 1 replica\nkubectl create deployment server --image nginx --replicas 1\n# Create a deployment for the client with nginx image and 1 replica\nkubectl create deployment client --image nginx --replicas 1\n# Expose the server deployment on port 80\nkubectl expose deployment server --port 80\n# Expose the client deployment on port 80\nkubectl expose deployment client --port 80\n# Get the pods labeled with app=client\nkubectl get pods -l app=client # client-&lt;...&gt;\n# Get the pods labeled with app=server\nkubectl get pods -l app=server # server-&lt;...&gt;\n# Copy the index3.html file to the server pod's nginx HTML directory\nkubectl cp index3.html server-&lt;...&gt;:usr/share/nginx/html/index.html\n\n# Execute the nginx reload command on the server pod\nkubectl exec server-&lt;...&gt; -- nginx -s reload\n\n# Open an interactive bash session in the client pod\nkubectl -i -t exec client-&lt;...&gt; -- bash\n\n# Get services labeled with app=server to find the cluster IP\nkubectl get services -l app=server # &lt;cluster-ip-server&gt;\n</code></pre>"},{"location":"plaintext/#searching-for-server-netnamespace","title":"Searching for server-&lt;...&gt; netnamespace:","text":"<p>Finding server's IP:</p> <pre><code># Find the server pod's IP address\nkubectl get pods -l app=server -o wide\n</code></pre> <p>Find and copy netnamespace <code>&lt;server-ns&gt;</code> having <code>&lt;server-ip&gt;</code>:</p> <pre><code># Find and copy the netnamespace &lt;server-ns&gt; corresponding to &lt;server-ip&gt;\nsudo ip -all netns exec ip -c a\n&lt;user&gt;@&lt;localhost&gt;:~$ sudo ip netns exec &lt;server-ns&gt; tcpdump -i eth0 -A port http\nroot@client-&lt;...&gt;:~# curl &lt;cluster-ip-server&gt;\n</code></pre> <p></p>"},{"location":"sniff/","title":"Sniff","text":"<p>Underneath commands are assumed to be executed under RHEL compatible OS, such as Fedora.</p> <p>Start by enabling some modules, then start some services and stop firewall:</p> <pre><code># Enable necessary modules\nsudo modprobe br-netfilter\n\n# Enable IP forwarding\nsudo sysctl net.ipv4.ip_forward=1\n# Enable bridge-netfilter calls for iptables\nsudo sysctl net.bridge.bridge-nf-call-iptables=1\n# Enable bridge-netfilter calls for ip6tables\nsudo sysctl net.bridge.bridge-nf-call-ip6tables=1\n# Start containerd service\nsudo systemctl start containerd\n\n# Stop firewalld service\nsudo systemctl stop firewalld\n</code></pre> <p>Clean after prev installations:</p> <pre><code># Clean up previous installations\nsudo kubeadm reset\nsudo rm /etc/cni/net.d/10-flannel.conflist\n\n# Initialize the Kubernetes cluster with a specific pod network CIDR\nsudo kubeadm init --pod-network-cidr=10.244.0.0/16\n\n# Remove existing kubeconfig files\nrm ~/.kube/*\n</code></pre> <p>Install k8s cluster:</p> <pre><code># Copy the Kubernetes admin configuration to the current user's kubeconfig\nsudo cp /etc/kubernetes/admin.conf ~/.kube/config\n\n# Change ownership of the kubeconfig file to the current user\nsudo chown &lt;user&gt;:&lt;user&gt; ~/.kube/config\n\n# Apply the flannel network configuration (replace &lt;flannel&gt; with the appropriate file)\nkubectl apply -f &lt;flannel&gt;\n\n# Taint the node to remove control-plane and master roles\nkubectl taint node fedora.36 node-role.kubernetes.io/control-plane-\nkubectl taint node fedora.36 node-role.kubernetes.io/master-\n</code></pre> <p>delete loop from config:</p> <pre><code># Edit the CoreDNS configuration map\nKUBE_EDITOR=vim kubectl -n kube-system edit configmap coredns\n\n# Restart the CoreDNS pods\nkubectl -n kube-system rollout restart coredns\n</code></pre>"},{"location":"sniff/#plaintext","title":"Plaintext","text":"<pre><code># Create a deployment for the server with nginx image and 1 replica\nkubectl create deployment server --image nginx --replicas 1\n# Create a deployment for the client with nginx image and 1 replica\nkubectl create deployment client --image nginx --replicas 1\n# Expose the server deployment on port 80\nkubectl expose deployment server --port 80\n# Expose the client deployment on port 80\nkubectl expose deployment client --port 80\n# Get the pods labeled with app=client\nkubectl get pods -l app=client # client-&lt;...&gt;\n# Get the pods labeled with app=server\nkubectl get pods -l app=server # server-&lt;...&gt;\n# Copy the index3.html file to the server pod's nginx HTML directory\nkubectl cp index3.html server-&lt;...&gt;:usr/share/nginx/html/index.html\n\n# Execute the nginx reload command on the server pod\nkubectl exec server-&lt;...&gt; -- nginx -s reload\n\n# Open an interactive bash session in the client pod\nkubectl -i -t exec client-&lt;...&gt; -- bash\n\n# Get services labeled with app=server to find the cluster IP\nkubectl get services -l app=server # &lt;cluster-ip-server&gt;\n</code></pre>"},{"location":"sniff/#searching-for-server-netnamespace","title":"searching for server-&lt;...&gt; netnamespace:","text":"<p>find server's IP:</p> <pre><code># Find the server pod's IP address\nkubectl get pods -l app=server -o wide\n</code></pre> <p>find and copy netnamespace \\ having \\: <pre><code># Find and copy the netnamespace &lt;server-ns&gt; corresponding to &lt;server-ip&gt;\nsudo ip -all netns exec ip -c a\n&lt;user&gt;@&lt;localhost&gt;:~$ sudo ip netns exec &lt;server-ns&gt; tcpdump -i eth0 -A port http\nroot@client-&lt;...&gt;:~# curl &lt;cluster-ip-server&gt;\n</code></pre>"},{"location":"sniff/#ciphertext","title":"Ciphertext","text":"<p>mesh application:</p> <pre><code># Perform pre-checks for Linkerd installation\nlinkerd check --pre\n\n# Install Linkerd control plane CRDs\nlinkerd install --crds | kubectl apply -f -\n\n# Install Linkerd control plane components\nlinkerd install | kubectl apply -f - # Takes approximately 50 seconds to start running, 70 seconds for all components to start\n# (Optional) Restart the linkerd-destination pod in the linkerd namespace\nkubectl -n linkerd rollout restart linkerd-destination\n\n# Perform post-installation checks for Linkerd\nlinkerd check\n</code></pre> <p>ignore warning:</p> <pre><code># Ignore warning by injecting Linkerd sidecar into server deployment YAML and applying the modified configuration\nkubectl get deployment server -o yaml | linkerd inject - | kubectl apply -f -\n\n# Ignore warning by injecting Linkerd sidecar into client deployment YAML and applying the modified configuration\nkubectl get deployment client -o yaml | linkerd inject - | kubectl apply -f -\n</code></pre> <p>linkerd is starting new pods, search for new pods and their netnamespaces: find new client's name:</p> <pre><code># Find the name of the new client pod\nkubectl get pods -l app=client\n</code></pre> <p>find new server's name:</p> <pre><code># Find the names of the new client and server pods\nkubectl get pods -l app=client\nkubectl get pods -l app=server\n\n# Copy the index3.html file to the server pod's nginx HTML directory using the nginx container\nkubectl -c nginx cp index3.html server-&lt;new...&gt;:usr/share/nginx/html/index.html\n\n# Execute the nginx reload command on the server pod using the nginx container\nkubectl -c nginx exec server-&lt;new...&gt; -- nginx -s reload\n</code></pre> <p>write down \\: <pre><code># Write down the IP address of the server pod\nkubectl get pods -l app=server -o wide\n</code></pre> <p>find and copy netnamespace \\ having \\: <pre><code># Find and copy the netnamespace &lt;server-new-ns&gt; corresponding to &lt;server-new-ip&gt;\nsudo ip -all netns exec ip -c a\nkubectl -c nginx -i -t exec client-&lt;new...&gt; -- bash\neng@fedora:~$ sudo ip netns exec &lt;server-new-ns&gt; tcpdump -i eth0 -A port http\nroot@client-&lt;new...&gt;:~# curl &lt;cluster-ip-server&gt;\n</code></pre>"},{"location":"solution_architecture/","title":"Solution architecture","text":""},{"location":"solution_architecture/#linkerd","title":"Linkerd","text":"<p>Linkerd is a service mesh for Kubernetes clusters. It is made out of two planes:</p> <ol> <li> <p>Data plane - the proxies 1, deployed alongside application code. These proxies handle the communication between the microservices and also act as a point at which the service mesh features can be introduced. Written in Rust. These proxies automatically handle all TCP traffic to and from the service, and communicate with the control plane for configuration.</p> </li> <li> <p>Control plane - is a set of services that and provide control over Linkerd as a whole. Written in Go.</p> </li> </ol> <p>inkerd also provides a CLI that can be used to interact with the control and data planes.</p> <p></p>"},{"location":"solution_architecture/#control-plane","title":"Control plane","text":"<p>The Linkerd control plane is a set of services that run in a dedicated Kubernetes namespace (linkerd by default). The control plane has several components, enumerated below.</p> <ol> <li> <p>The destination service The destination service is used by the data plane proxies to determine various aspects of their behavior. It is used to fetch service discovery information (i.e. where to send a particular request and the TLS identity expected on the other end); to fetch policy information about which types of requests are allowed; to fetch service profile information used to inform per-route metrics, retries, and timeouts; and more.</p> </li> <li> <p>The identity service The identity service acts as a TLS Certificate Authority that accepts CSRs from proxies and returns signed certificates. These certificates are issued at proxy initialization time and are used for proxy-to-proxy connections to implement mTLS.</p> </li> <li> <p>The proxy injector The proxy injector is a Kubernetes admission controller that receives a webhook request every time a pod is created. This injector inspects resources for a Linkerd-specific annotation (linkerd.io/inject: enabled). When that annotation exists, the injector mutates the pod\u2019s specification and adds the proxy-init and linkerd-proxy containers to the pod, along with the relevant start-time configuration.</p> </li> </ol>"},{"location":"solution_architecture/#data-plane","title":"Data plane","text":"<p>The Linkerd data plane comprises ultralight micro-proxies which are deployed as sidecar containers inside application pods. These proxies transparently intercept TCP connections to and from each pod, thanks to iptables rules put in place by the linkerd-init (or, alternatively, by Linkerd\u2019s CNI plugin).</p>"},{"location":"solution_architecture/#proxy","title":"Proxy","text":"<p>The Linkerd2-proxy is an ultralight, transparent micro-proxy written in Rust. Linkerd2-proxy is designed specifically for the service mesh use case and is not designed as a general-purpose proxy.</p> <p>The proxy\u2019s features include:</p> <ul> <li>Transparent, zero-config proxying for HTTP, HTTP/2, and arbitrary TCP protocols.</li> <li>Automatic Prometheus metrics export for HTTP and TCP traffic.</li> <li>Transparent, zero-config WebSocket proxying.</li> <li>Automatic, latency-aware, layer-7 load balancing.</li> <li>Automatic layer-4 load balancing for non-HTTP traffic.</li> <li>Automatic TLS.</li> <li>An on-demand diagnostic tap API.</li> </ul> <p>The proxy supports service discovery via DNS and the destination gRPC API.</p> <p>2</p> <ol> <li> <p>Eliza Weisman. Under the hood of linkerd's state-of-the-art rust proxy, linkerd2-proxy. July 23, 2020. URL: https://linkerd.io/2020/07/23/under-the-hood-of-linkerds-state-of-the-art-rust-proxy-linkerd2-proxy/.\u00a0\u21a9</p> </li> <li> <p>Architecture. URL: https://linkerd.io/2.13/reference/architecture/#control-plane.\u00a0\u21a9</p> </li> </ol>"},{"location":"summary/","title":"Summary - conclusions","text":"<p>In this demonstration, we compared the communication security between plaintext and ciphertext using Linkerd service mesh. Without the usage of Linkerd, there is a risk of eavesdropping, where a malicious entity could sniff the communication and potentially access sensitive data. However, with the implementation of Linkerd and its proxy sidecars, communication is encrypted using mutual TLS (mTLS), significantly lowering the risk of data exposure.</p> <p>To showcase the difference, we set up two services, Service A and Service B, within a Kubernetes cluster. We then used ksniff and Wireshark to record the communication between the pods and attempted to observe the data sent. In the first scenario without Linkerd, the communication was in plaintext, making it vulnerable to eavesdropping. However, after deploying Linkerd and enabling encryption, we repeated the experiment. This time, the observed communication was encrypted, making it impossible to read the transmitted data.</p> <p>The demonstration highlights the importance of using a service mesh like Linkerd to enhance the security of communication within a Kubernetes environment. By leveraging mTLS encryption and proxy sidecars, Linkerd ensures that sensitive data remains protected and inaccessible to unauthorized entities.</p>"},{"location":"summary/#visible-plaintext","title":"Visible plaintext:","text":""},{"location":"summary/#ciphertext","title":"Ciphertext:","text":""},{"location":"team_and_contribution/","title":"Team and contribution","text":""},{"location":"team_and_contribution/#team-consists-of-4-members","title":"Team consists of 4 members:","text":"<ul> <li>Piotr Kica</li> <li>Micha\u0142 Grzelak</li> <li>Antoni Gradowski</li> <li>Jan Bugajski</li> </ul>"},{"location":"team_and_contribution/#individual-contribution-to-project","title":"Individual contribution to project","text":""},{"location":"team_and_contribution/#piotr-kica","title":"Piotr Kica","text":"<p>Created and managed the GitHub repository throughout the duration of the project. He was the first one who installed and successfully tested Linkerd on AWS instance. Moreover, Piotr came up with an idea of testing fault injection and observability guaranteed by Linkerd.</p>"},{"location":"team_and_contribution/#antoni-gradowski","title":"Antoni Gradowski","text":"<p>Wrote draft layout of the documentation.</p>"},{"location":"team_and_contribution/#micha-grzelak","title":"Micha\u0142 Grzelak","text":"<p>Added the first two chapters to this document, started third chapter and was responsible for transforming documentation's layout into \\LaTeX. Furthermore, Micha\u0142 came up with an idea of sniffing inter-service communication without Linkerd.</p>"},{"location":"team_and_contribution/#jan-bugajski","title":"Jan Bugajski","text":"<p>Was responsible for cleaning up and writing down ideas risen during brainstorms. Was responsible for expanding documentation and formatting Mkdocs.</p>"},{"location":"zbibliography/","title":"Bibliography","text":"<ol> <li> <p>RedHat. What's a service mesh? June 29, 2018. URL: https://www.redhat.com/en/topics/microservices/what-is-a-service-mesh.\u00a0\u21a9</p> </li> <li> <p>William Morgan. Announcing the completion of linkerd's 2022 security audit. June 27, 2022. URL: https://linkerd.io/2022/06/27/announcing-the-completion-of-linkerds-2022-security-audit.\u00a0\u21a9</p> </li> <li> <p>Eldad Rudich. Ksniff. January 30, 2023. URL: https://github.com/eldadru/ksniff?fbclid=IwAR0_3IuIv36cqxW_xFZxbSsOTCjGRvST5kADJvqt53si9hvqveG4ShJLdm4.\u00a0\u21a9</p> </li> <li> <p>Getting started. URL: https://linkerd.io/2.13/getting-started/.\u00a0\u21a9</p> </li> <li> <p>Eliza Weisman. Under the hood of linkerd's state-of-the-art rust proxy, linkerd2-proxy. July 23, 2020. URL: https://linkerd.io/2020/07/23/under-the-hood-of-linkerds-state-of-the-art-rust-proxy-linkerd2-proxy/.\u00a0\u21a9</p> </li> <li> <p>Architecture. URL: https://linkerd.io/2.13/reference/architecture/#control-plane.\u00a0\u21a9</p> </li> </ol>"}]}